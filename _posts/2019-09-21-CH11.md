---
layout: post
title: 第十一章 一元线性回归
date: 2019-09-21
Author: 杨舒文
categories: [统计学, 统计学简答]
tags: [统计学]
comments: true

---

### 1. 简述一元回归模型的基本假设。`important`

答：（1）一元线性回归模型的概念  
对于具有线性关系的两个变量，可以用一元线性方程来表示它们之间的关系。描述因变量y如何依赖于自变量x和误差项ε的方程称为回归模型。只涉及一个自变量的一元线性回归模型，可表示为：$y=\beta_0+\beta_1x+\varepsilon$。   
（2）一元线性回归模型的主要假设  
① 因变量y与自变量x之间具有线性关系；  
② 在重复抽样中，自变量x的取值是固定的，即假定x是非随机的；  
③ 误差项ε是一个期望值为0的随机变量，即E（ε）＝0；  
④ 对于所有的x值，ε的方差$\sigma^2$都相同；   
⑤ 误差项ε是一个服从正态分布的随机变量，且独立，即ε～N（0，$\sigma^2$）。   

> 模型参数的最小二乘估计$\hat a$和$\hat b$具有哪些统计特性？若模型用于预测，影响预测精度的因素有哪些？
>
> - 统计特性：
>
> 1. 线性，即估计量$\hat a$和$\hat b$为随机变量$y_i$的线性函数；
> 2. 无偏性，$\hat a$和$\hat b$分别是截距系数a和斜率系数b的无偏估计；
> 3. 有效性，$\hat a$和$\hat b$是所有线性无偏估计量中具有最小方差的估计量。
>
> - 影响因素：
>
> 1. 预测区间置信度越高，预测区间越宽，精度越低；
> 2. 总体y分布的离散程度$\sigma^2$越大，预测区间越宽，精度越低；
> 3. 样本观测量n，n越大，预测区间越窄，精度越高；
> 4. 样本观测点中，解释变量x分布的离散度，x分布越离散，精度越高；
> 5. 预测点$x_0$离样本分布中心$\bar x$的距离，越接近中心点，预测区间越窄，精度越高。



### 2. 简述回归分析的一般过程。

答：回归分析是确定两个或两个以上变量间相互依赖的定量关系的一种统计分析方法，进行回归分析的一般过程为：  
（1）明确预测的具体目标，确定因变量和自变量。  
（2）进行相关分析。回归分析是对具有因果关系的影响因素（自变量）和预测对象（因变量）所进行的数理统计分析处理。只有当自变量与因变量确实存在某种关系时，建立的回归方程才有意义。因此，作为自变量的因素与作为因变量的预测对象是否有关，相关程度如何，以及判断这种相关程度的把握性多大，是进行回归分析必须要解决的问题。进行相关分析，一般要求出相关系数，以相关系数的大小来判断自变量和因变量的相关的程度。  
（3）建立预测模型。依据自变量和因变量的历史统计资料进行计算，在此基础上建立回归分析方程。  
（4）计算预测误差。回归预测模型是否可用于实际预测，取决于对回归预测模型的检验和对预测误差的计算。回归方程只有通过各种检验，且预测误差较小，才能将回归方程作为预测模型进行预测。  
（5）确定预测值。利用回归预测模型计算预测值，并对预测值进行综合分析，确定最后的预测值。



### 3. 试问独立性与不相关之间的区别与联系？

答：对于事件A和B，若P（AB）＝P（A）P（B），则称事件A和事件B相互独立，简称A与B独立。对于n个事件A1，A2，…An，若对于所有可能组合1≤i＜j＜k＜…≤n，以下等式均成立 

> $P(A_iA_j)=P(A_i)P(A_j)$
>
> $P(A_iA_jA_k)=P(A_i)P(A_j)P(A_k)$
>
> $·····$
>
> $P(A_1A_2···A_n)=P(A_1)P(A_2)···P(A_n)$

则称A1，A2，…An相互独立；若只有第一个等式成立，则称A1，A2，…An两两独立。
对于随机变量X与Y，若Var（X）＞0，Var（Y）＞0，则称

> $\dfrac{Cov(X,Y)}{\sqrt {Var(X)Var(Y)}}$

为X与Y的相关系数，记为ρ或ρXY。若ρXY＝0，则称随机变量X与Y不相关。

- 两者的联系：独立性和不相关性都是随机变量间联系“薄弱”的一种反映。若X与Y独立，则X与Y不相关。在二维正态分布中，不相关性与独立性是等价的。  
- 两者的区别：两个随机变量相互独立与不相关是两个不同的概念，不相关只说明两个随机变量之间没有线性关系，但这时的X与Y可能有某种别的函数关系；而相互独立说明两个随机变量之间没有任何关系，既没有线性关系，也没有其他关系。



### 4. 给出在一元线性回归中：（1）相关系数的定义和直观意义；（2）判定系数的定义和直观意义；（3）相关系数和判定系数的关系。

答：（1）相关系数是根据样本数据计算的，用来度量两个变量之间线性关系强度的统计量。样本相关系数的计算公式为：  

&ensp;&ensp;$r = \dfrac {\sum xy-n\bar x\bar y}{\sqrt{\sum x^2-n\bar x^2}\sqrt{\sum y^2-n\bar y^2}}=\dfrac{l_{xy}}{\sqrt{l_{xx}l_{yy}}}$  

相关系数r仅仅是x与y之间线性关系的一个度量，它不能用于描述非线性关系。r的取值在－1到1之间，当r的绝对值越接近于1时，x与y之间线性相关关系越强；r＝0只表示两个变量之间不存在线性相关关系，并不说明变量之间没有任何关系，它们之间可能存在非线性相关关系。变量之间的非线性相关程度较大时，就可能会导致r＝0。因此，当r＝0或很小时，不能轻易得出两个变量之间不存在相关关系的结论，而应结合散点图做出合理的解释。  
（2）回归平方和占总平方和的比例称为判定系数，记为R2，其计算公式为：  

&ensp;&ensp;$R^2=\dfrac{SSR}{SST}=\dfrac{\sum(\hat y_i-\bar y)^2}{\sum(y_i-\bar y)^2}=1-\dfrac{\sum(y_i-\hat y_i)^2}{\sum(y_i-\bar y)^2}$  

判定系数R2测度了回归直线对观测数据的拟合程度。R2的取值范围是[0，1]。R2越接近于1，表明回归平方和占总平方和的比例越大，回归直线与各观测点越接近，用x的变化来解释y值变差的部分就越多，回归直线的拟合程度就越好；反之，R2越接近于0，回归直线的拟合程度就越差。  
（3）在一元线性回归中，相关系数r实际上是判定系数的平方根，其正负符号与回归方程中回归系数的符号相同。



### 5. 利用相关系数如何判断变量之间相关的方向和相关关系的密切程度？

答：相关系数r的取值范围在－1～＋1之间。若0＜r≤1，表明变量x与y之间存在正线性相关关系；若－1≤r＜0，表明x与y之间存在负线性相关关系；若r＝＋1，表明x与y之间为完全正线性相关关系；若r＝－1，表明x与y之间为完全负线性相关关系，可见当\|r\|＝1时，y的取值完全依赖于x，二者之间即为函数关系；当r＝0时，说明两者之间不存在线性相关关系，但可能存在其他非线性相关关系。  
\|r\|→1说明两个变量之间的线性关系越强；\|r\|→0说明两个变量之间的线性关系越弱。对于一个具体的r取值，根据经验可将相关程度分为以下几种情况：当\|r\|≥0.8时，可视为高度相关；当0.5≤\|r\|＜0.8时，可视为中度相关；当0.3≤\|r\|＜0.5时，视为低度相关；当\|r\|＜0.3时，说明两个变量之间的相关程度极弱，可视为不相关。但这种解释必须建立在对相关系数的显著性检验的基础之上。  



### 6. 概述相关分析与回归分析的联系与区别。

答：（1）相关分析和回归分析的联系：  
它们具有共同的研究对象，都是对变量间相关关系的分析，二者可以相互补充。相关分析可以表明变量间相关关系的性质和程度，只有当变量间存在相当程度的相关关系时，进行回归分析去寻求变量间相关的具体数学形式才有实际的意义。同时，在进行相关分析时，如果要具体确定变量间相关的具体数学形式，又要依赖于回归分析，而且在多个变量的相关分析中相关系数的确定也是建立在回归分析基础上的。  
（2）相关分析和回归分析的区别：  
① 从研究目的上看，相关分析是用一定的数量指标（相关系数）度量变量间相互联系的方向和程度；回归分析却是要寻求变量间联系的具体数学形式，是要根据自变量的给定值去估计和预测因变量的平均值。  
② 从对变量的处理看，相关分析对称地对待相互联系的变量，不考虑二者的因果关系，也就是不区分自变量和因变量，相关的变量不一定具有因果关系，均视为随机变量。回归分析是在变量因果关系分析的基础上研究其中的自变量的变动对因变量的具体影响，必须明确划分自变量和因变量，所以回归分析中对变量的处理是不对称的，在回归分析中通常假定自变量在重复抽样中是取固定值的非随机变量，只有因变量是具有一定概率分布的随机变量。



### 7. 若有线性回归模型$y_t=\beta_1+\beta_2x_t+\varepsilon_t$（t＝1，2，…，n），其中E（$\varepsilon_t$）＝0，E（$\varepsilon_t^2$）＝$\sigma^2x_t^2$，E（$\varepsilon_t \varepsilon_s$）＝0（t≠s），问：（1）该模型是否违背古典线性回归模型的假定，请简要说明；（2）如果对该模型进行估计，你会采用什么方法？请说明理由。

答：（1）该模型违背了古典线性回归模型的假定。古典线性回归模型要求误差项具有等方差性，即对于不同的自变量x具有相同的方差。而由题意可知，误差项$\varepsilon_t$的方差为$\sigma^2x_t^2$，与自变量有关。  
（2）如果对该模型进行估计，会采用加权最小二乘法。加权最小二乘法是在平方和中加入权数$1/x_t^2$，以调整各项在平方和中的作用，即寻找参数$\beta_1,\beta_2$的估计值$\hat \beta_1, \hat \beta_2$，使得离差平方和  

&ensp;&ensp;$Q(\beta_1,\beta_2)=\sum\limits_{t=1}^n \dfrac{1}{x_t^2}(y_t-\beta_1x_t-\beta_0) $  

达到最小。这样，就消除了异方差性的影响。



### 8. 回归分析结果的评价。

答：对回归分析结果的评价可以从以下四个方面入手：  
（1）所估计的回归系数$\hat \beta_1$的符号是否与理论或事先预期相一致；  
（2）如果理论上认为y与x之间的关系不仅是正的，而且是统计上显著的，那么所建立的回归方程也应该如此；  
（3）用判定系数$R^2$来回答回归模型在多大程度上解释了因变量y取值的差异；  
（4）考察关于误差项ε的正态性假定是否成立。因为在对线性关系进行F检验和对回归系数进行t检验时，都要求误差项ε服从正态分布，否则，所用的检验程序将是无效的。检验ε正态性的简单方法是画出残差的散点图或正态概率图。  



### 9. 什么是置信区间估计和预测区间估计？二者有何区别？

答：（1）置信区间估计，它是对x的一个给定值$x_0$，求出y的平均值的估计区间，这一区间称为置信区间；  
预测区间估计，它是对x的一个给定值$x_0$，求出y的一个个别值的估计区间，这一区间称为预测区间。  
（2）置信区间估计和预测区间估计的区别：置信区间估计是求y的平均值的估计区间，而预测区间估计是求y的一个个别值的估计区间；对同一个$x_0$，这两个区间的宽度也是不一样的，预测区间要比置信区间宽一些。
