---
layout: post
title: 第十二章 多元线性回归 
date: 2019-09-21
Author: 杨舒文
categories: [统计学, 统计学简答]
tags: [统计学]
comments: true

---

### 1. 简述多元线性回归模型中存在高度多重共线性的后果，常用的检验方法以及补救办法。`important`

答：当回归模型中两个或两个以上的自变量彼此相关时，则称回归模型中存在多重共线性。  
（1）多元线性回归模型中存在高度多重共线性产生的后果  
① 变量之间高度相关时，可能会使回归的结果混乱，甚至会把分析引入歧途。  
② 多重共线性可能对参数估计值的正负号产生影响，特别是$\beta_i$的正负号有可能同预期的正负号相反。  
（2）多重共线性常用的检验方法  
① 计算模型中各对自变量之间的相关系数，并对各相关系数进行显著性检验。如果有一个或多个相关系数是显著的，就表示模型中所使用的自变量之间相关，因而存在多重共线性问题。  
② 经验判别。具体来说，如果出现下列情况，暗示存在多重共线性：  

> a．模型中各对自变量之间显著相关。  
> b．当模型的线性关系检验（F检验）显著时，几乎所有回归系数βi的t检验却不显著。  
> c．回归系数的正负号与预期的相反。  
> d．容忍度与方差扩大因子（VIF）。某个自变量的容忍度等于1减去该自变量为因变量而其他k－1个自变量为预测变量时所得到的线性回归模型的判定系数，即$1-R_i^2$。容忍度越小，多重共线性越严重。通常认为容忍度小于0.1时，存在严重的多重共线性。方差扩大因子等于容忍度的倒数，即$VIF=1/(1-R_i^2)$。显然，VIF越大，多重共线性越严重。一般认为VIF大于10时，存在严重的多重共线性。	  

（3）多重共线性的补救办法  
① 将一个或多个相关的自变量从模型中剔除，使保留的自变量尽可能不相关。  
② 如果要在模型中保留所有的自变量，那就应该避免根据t统计量对单个参数β进行检验，并且对因变量y值的推断（估计或预测）限定在自变量样本值的范围内。



### 2. 说明回归模型的假设以及当这些假设不成立时的应对方法。

答：（1）多元回归模型的基本假定有：  
① 自变量$x_1$，$x_2$，…，$x_k$是非随机的、固定的，且相互之间互不相关（无多重共线性）；  
② 误差项ε是一个期望值为0的随机变量，即E（ε）＝0；  
③ 对于自变量$x_1$，$x_2$，…，$x_k$的所有值，ε的方差$\sigma^2$都相同，且不序列相关，即D（$\varepsilon_i$）＝$\sigma^2$，Cov（$\varepsilon_i$，$\varepsilon_j$）＝0，i≠j；  
④ 误差项ε是一个服从正态分布的随机变量，且相互独立，即ε～N（0，$\sigma^2$）。   
（2）若模型中存在**多重共线性**时，解决的方法有：  
第一，将一个或多个相关的自变量从模型中剔除，使保留的自变量尽可能不相关。  
第二，如果要在模型中保留所有的自变量，那就应该：避免根据t统计量对单个参数β进行检验；对因变量Y值的推断（估计或预测）限定在自变量样本值的范围内。  
当随机误差项**ε不相互独立**时，则说明回归模型存在序列相关性，这时首先要查明序列相关产生的原因：如果是回归模型选用不当，则应改用适当的回归模型；如果是缺少重要的自变量，则应增加自变量；如果以上两种方法都不能消除序列相关性，则需采用迭代法、差分法等方法处理。  
当存在**异方差性**时，普通最小二乘估计不再具有最小方差线性估计的性质，这时可以采用加权最小二乘法改进估计的性质。加权最小二乘估计对误差项方差小的项加一个大的权数，对误差项方差大的项加一个小的权数，因此加强了小方差项的地位，使离差平方和中各项的作用相同。  



### 3. 多元回归分析中为什么需要使用修正的判定系数（可决系数）来比较方程的拟合效果？是如何计算的？

答：在多元线性回归分析中，常用修正的判定系数，而不用多重判定系数来衡量估计模型对样本观测值的拟合优度。这是由于在样本容量不变的情况，随着样本解释变量个数的增加，多重判定系数$R^2$的值会越来越高（即$R^2$是解释变量个数的增函数）。因为在模型中增加新的解释变量不会改变总离差平方和，但可能增加回归平方和，减少残差平方和，从而改变模型的解释功能。但是解释变量个数的增加会加大模型估计的工作量，且加入模型中的解释变量不一定具有现实意义，因此在多元线性回归模型之间比较拟合优度时，$R^2$不是一个合适的指标，需用样本容量n和自变量的个数k加以调整。而修正判定系数$R^2_a$对模型中解释变量个数的增加施加了约束，因此在用于估计多元回归模型方面要优于多重判定系数$R^2$。其计算公式为  

&ensp;&ensp;$\begin{aligned}R_a^2=1-(1-R^2)\frac{n-1}{n-k-1} =1-\frac{SSE/(n-k-1)}{SST/(n-1)}\end{aligned}$

> **解释多重判定系数和调整的多重判定系数的含义和作用。**
>
> 答：（1）多重判定系数是多元回归中的回归平方和占总平方和的比例，它是度量多元回归方程拟合程度的一个统计量，反映了在因变量y的变差中被估计的回归方程所解释的比例，其计算公式为：$ R^2$＝SSR/SST＝1－SSE/SST。
> （2）调整的多重判定系数考虑了样本量（n）和模型中自变量的个数（k）的影响，这就使得$R_a^2$的值永远小于$ R^2$，而且$ R_a^2$的值不会由于模型中自变量个数的增加而越来越接近1，其计算公式为：
>
> ​		$\begin{aligned}R_a^2=1-(1-R^2)\frac{n-1}{n-k-1} =1-\frac{SSE/(n-k-1)}{SST/(n-1)}\end{aligned}$



### 4. 在多元线性回归中，为什么我们对整个回归方程进行检验后，还要对每个回归系数进行检验呢？

答：在多元线性回归中，线性关系检验主要是检验因变量同多个自变量的线性关系是否显著，在k个自变量中，只要有一个自变量与因变量的线性关系显著，F检验就能通过，但这不一定意味着每个自变量与因变量的关系都显著。回归系数检验则是对每个回归系数分别进行单独的检验，它主要用于检验每个自变量对因变量的影响是否都显著。如果某个自变量没有通过检验，就意味着这个自变量对因变量的影响不显著，也许就没有必要将这个自变量放进回归模型中了。另外，通过该步骤还可以初步判断自变量间是否存在多重共线性：当某些重要的自变量的回归系数t检验不通过而同时整个回归方程的线性关系检验又能通过时，则通常预示着自变量间存在多重共线性。



### 5. 解释多元回归模型、多元回归方程、估计的多元回归方程的含义。

答：（1）多元回归模型：设因变量为y，k个自变量分别为$ x_1 $，$ x_2 $，…，$ x_k $，描述因变量y如何依赖于自变量$ x_1 $，$ x_2 $，…，$ x_k $和误差项ε的方程称为多元回归模型。其一般形式可表示为：y＝$ \beta_0 $＋$ \beta_1  x_1$ ＋$ \beta_2  x_2 $＋…＋$ \beta_k  x_k $＋ε，式中，$\beta_0 $，$ \beta_1 $，$ \beta_2 $，…，$ \beta_k $是模型的参数，ε为误差项。  
（2）多元回归方程：根据回归模型的假定有E（y）＝$ \beta_0 $＋$ \beta_1  x_1 $＋$ \beta_2  x_2 $＋…＋$ \beta_k  x_k $，称为多元回归方程，它描述了因变量y的期望值与自变量$ x_1 $，$ x_2 $，…，$ x_k $之间的关系。  
（3）估计的多元回归方程：回归方程中的参数$ \beta_0 $，$ \beta_1 $，$ \beta_2 $，…，$ \beta_k $是未知的，需要利用样本数据去估计它们。当用样本统计量$\hat {\beta_0}$，$\hat {\beta_1} $，$\hat {\beta_2}$，…，$\hat {\beta_k}$去估计回归方程中的未知参数$ \beta_0 $，$ \beta_1 $，$ \beta_2 $，…，$ \beta_k $时，就得到了估计的多元回归方程，其一般形式为：$\hat y$＝$\hat {\beta_0}$＋$\hat {\beta_1} x_1 $＋$\hat {\beta_2} x_2 $＋…＋$\hat {\beta_k} x_k $式中，$\hat {\beta_0}$，$\hat {\beta_1}$，$\hat {\beta_2}$，…，$\hat {\beta_k}$是参数$ \beta_0 $，$ \beta_1 $，$ \beta_2 $，…，$ \beta_k $的估计值，$\hat {y}$是因变量y的估计值。$\hat {\beta_0}$，$\hat {\beta_1}$，$\hat {\beta_2}$，…，$\hat {\beta_k}$称为偏回归系数。  



