---
layout: post
title: RNN系列总结
date: 2021-03-20 17:01:00 +0800
Author: 杨舒文	
categories: [NLP, RNN] 
tags: [NLP, GRU, LSTM]
comments: true
---

## RNN

1. RNN原理

   数学表达式：

   $$ \pmb h_t :=tanh(\pmb W_{xh}\pmb x_t+\pmb W_{hh}\pmb h_{t-1})$$

   简化：

   $${\pmb h}_t := \pmb f(\pmb x_t,\pmb h_{t-1})$$

   电路图如下：

   ![](/assets/img/md_pictrue/RNN.png)

2. 梯度爆炸与梯度消失

   假设激活函数为恒等变换，当t很大时，目标偏导数取决于对角阵最大值$\lambda_1$大于1还是小于1。大于1时，发生梯度爆炸；小于1时，发生梯度消失。**梯度爆炸**可以用梯度裁剪（gradient clipping）来解决。LSTM和GRU通过门（gate）机制控制RNN中的信息流动，用来缓解**梯度消失**问题。



## LSTM

LSTM（Long Short-Term Memory）由Hochreiter和Schmidhuber提出。

数学表达式：

$$\pmb i_t :=sigmoid(\pmb W_{xi}\pmb x_t+\pmb W_{hi}\pmb h_{t-1})$$

$$\pmb f_t :=sigmoid(\pmb W_{xf}\pmb x_t+\pmb W_{hf}\pmb h_{t-1})$$

$$\pmb o_t :=sigmoid(\pmb W_{xo}\pmb x_t+\pmb W_{ho}\pmb h_{t-1})$$

$$\pmb {\tilde c_t} :=tanh(\pmb W_{xc}\pmb x_t+\pmb W_{hc}\pmb h_{t-1})$$

$$\pmb c_t :=\pmb f_t \odot \pmb c_{t-1}+\pmb i_t \odot \tilde{\pmb c}_t$$

$$\pmb h_t := \pmb o_t \odot tanh(\pmb c_t)$$

其中，$\pmb i$代表**输入门**（input），$\pmb f$代表**遗忘门**（forget），$\pmb o$代表**输出门**（output）。和RNN相比，LSTM多了一个隐状态变量$\pmb c_t$，称为细胞状态（cell state）。

简化：

$$\pmb {\tilde c_t} :=tanh(\pmb W_{xc}\pmb x_t+\pmb W_{hc}\pmb h_{t-1})$$

$$\pmb c_t :=\pmb f_t \cdot \pmb c_{t-1}+\pmb i_t \cdot \tilde{\pmb c}_t$$

$$\pmb h_{t-1} := \pmb o_{t-1} \cdot tanh(\pmb c_{t-1})$$

![](/assets/img/md_pictrue/LSTM.png)

当 $i_t=1$（闭合）、$f_t=0$（打开）、$o_t=1$（闭合）时，LSTM退化为标准的RNN。



## GRU

数学表达式：

$$\pmb z_t :=sigmoid(\pmb W_{xz}\pmb x_t+\pmb W_{hz}\pmb h_{t-1})$$

$$\pmb r_t :=sigmoid(\pmb W_{xr}\pmb x_t+\pmb W_{hr}\pmb h_{t-1})$$

$$\pmb {\tilde h_t} :=tanh(\pmb W_{xh}\pmb x_t+\pmb r_t \odot (\pmb W_{hh}\pmb h_{t-1}))$$

$$\pmb h_t :=\pmb (1-\pmb z_t)\odot \pmb {\tilde h_t}+\pmb z_t \odot \pmb h_{t-1}$$

简化：

$$\pmb {\tilde h_t} :=tanh(\pmb W_{xh}\pmb x_t+\pmb r_t \cdot (\pmb W_{hh}\pmb h_{t-1}))$$

$$\pmb h_t :=\pmb (1-\pmb z_t)\cdot \pmb {\tilde h_t}+\pmb z_t \cdot \pmb h_{t-1}$$

![](/assets/img/md_pictrue/GRU.png)

与LSTM相比，GRU将输入们和遗忘门融合成单一的**更新门**$z_t$，融合了细胞状态和隐层单元，$r_t$为**重置门**。当$r_t=1$（闭合）、$z_t=0$（开关连上面）GRU退化为标准的RNN。



## 小结

RNN、LSTM、GRU输出都是取$h_t$。只不过LSTM在循环过程中，多携带了一个细胞状态$c_t$。三者的计算单元基本一致，都是对$x_t$和$h_t$做线性映射然后加激活函数。区别在于如何设计门控缓解梯度消失。